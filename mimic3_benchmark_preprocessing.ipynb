{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelal-Eizaldeen/DLH-Project-Reproduce-HurtfulWords/blob/main/mimic3_benchmark_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Names: Payel Chakraborty && Zilal Eiz Al Din\n",
        "\n",
        "NetIDs: payelc2 && zelalae2\n",
        "\n",
        "Purpose:\n",
        "This file contains Paper Name, their Pretrained Models, Environment and Prerequisites and Data processing.\n",
        "\n",
        "Dataset:\n",
        "HurtfulWordsDataset (MIMIC-III)\n",
        "\n",
        "Paper Reference:\n",
        "Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings\n",
        "\n",
        "The paper: https://arxiv.org/abs/2003.11515\n",
        "\n",
        "Github Repo of the Original paper: https://github.com/MLforHealth/HurtfulWords\n",
        "\n",
        "Github Repo of the Reproduction: https://github.com/zelal-Eizaldeen/DLH-Project-Reproduce-HurtfulWords\n",
        "\n",
        "However, these steps have been obtainwd from https://github.com/YerevaNN/mimic3-benchmarks  to build the benchmark. It assumes that you already have MIMIC-III dataset (lots of CSV files) on the disk.\n",
        "\n",
        "Usage: From the Github link mentioend above, \"After all the below commands are done, there will be a directory data/{task} for each created benchmark task. These directories have two subdirectories: train and test. Each of them contains a bunch of ICU stays and one file with name listfile.csv, which lists all samples in that particular set. Each row of listfile.csv has the following form: icu_stay, period_length, label(s). A row specifies a sample for which the input is the collection of ICU event of icu_stay that occurred in the first period_length hours of the stay and the target is/are label(s). In in-hospital mortality prediction task period_length is always 48 hours, so it is not listed in corresponding listfiles.\"\n",
        "\n",
        "Please note that extract_subjects.py has been changed as per our requirement, so it has been uploaded to our Github of reproduction."
      ],
      "metadata": {
        "id": "HCohhtG0QBJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# These steps have been run to create the \"mimic-benchmark\" directory and its content as per the directions on https://github.com/YerevaNN/mimic3-benchmarks."
      ],
      "metadata": {
        "id": "7o5Cqf44bHfC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZgy5hfZQ78o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting the Google Colab"
      ],
      "metadata": {
        "id": "Zf5CPvkrbI_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlO366NDa3J0",
        "outputId": "4cdf0a22-e7c8-4db1-dab6-f740942c4590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97gm3cfm6zqD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#running the script from my path /content/drive/MyDrive/Payel-DLH-related. Then after Git clone, it creates the folder mimic3-benchmarks from where the remaining steps will run. If you are running the script, clone from the path wherever you want the \"mimic3-benchmarks\" folder to be."
      ],
      "metadata": {
        "id": "-6FC48J7foyO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl1dy7FRFvFy",
        "outputId": "cf82dbaa-14b6-471d-cbf4-1d560b3feed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mimic3-benchmarks'...\n",
            "remote: Enumerating objects: 1898, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 1898 (delta 58), reused 34 (delta 34), pack-reused 1820 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1898/1898), 16.94 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (1308/1308), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/YerevaNN/mimic3-benchmarks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiewhjVrGAgU",
        "outputId": "f9030c31-9b28-4bcc-e141-951479fe8fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Payel-DLH-related/mimic3-benchmarks\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/Payel-DLH-related/mimic3-benchmarks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cNBVWLzXLsX",
        "outputId": "39468efa-51ca-4fdc-bbf2-1aa59f8abd19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique subjects: 1411\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Load your file (replace with your actual file path)\n",
        "# df = pd.read_csv('/content/drive/MyDrive/Payel-DLH-related/DataFiles/cleaned_data/data.csv')\n",
        "\n",
        "# # Replace 'subject_id' with the actual column name that identifies subjects\n",
        "# num_unique_subjects = df['subject_id'].nunique()\n",
        "# print(\"Number of unique subjects:\", num_unique_subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npUD12b7paAc"
      },
      "source": [
        "#Benchmark Execution Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before running this, bring in the requirements.txt from our github path: DLH-Project-Reproduce-HurtfulWords/mimic3-benchmark/"
      ],
      "metadata": {
        "id": "LqQG4eNlmoLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6225lEoYO7l4",
        "outputId": "15cf944c-6d2a-4b4a-a842-3c187094aff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Keras in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (5.29.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from Keras->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from Keras->-r requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from Keras->-r requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from Keras->-r requirements.txt (line 1)) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from Keras->-r requirements.txt (line 1)) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from Keras->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from Keras->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 2)) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->Keras->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->Keras->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->Keras->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 2)) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt5qQFKgJC1u"
      },
      "source": [
        "# 1-The following command takes MIMIC-III CSVs, generates one directory per SUBJECT_ID and writes ICU stay information to data/{SUBJECT_ID}/stays.csv, diagnoses to data/{SUBJECT_ID}/diagnoses.csv, and events to data/{SUBJECT_ID}/events.csv. This step might take around an hour.\n",
        "# Please note that the extract_subjects.py script has been changed in this context to retain only subjects that are within our \"data.csv\" (output of the script filtering_tokenizing_notes.ipynb), that contains patients information with notes filtered to \"Nursing\", \"Nursing/other\", \"Physician\"\", and Discharge Summary\" and restricted to the first 48 hours of ICU stay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGX6PL_TJWHp"
      },
      "outputs": [],
      "source": [
        "#The original unsampled data files --change it to your path where the raw unzipped files are there\n",
        "PATH_TO_CSVs = '/content/drive/MyDrive/Payel-DLH-related/DataFiles/mimic-iii-clinical-database-1.4'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMT6UC19Jcd1",
        "outputId": "fda0140c-42eb-41ac-fa34-dc1a18b72bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADMISSIONS.csv\t\t   D_ICD_PROCEDURES.csv    OUTPUTEVENTS.csv\n",
            "CALLOUT.csv\t\t   D_ITEMS.csv\t\t   PATIENTS.csv\n",
            "CAREGIVERS.csv\t\t   D_LABITEMS.csv\t   PRESCRIPTIONS.csv\n",
            "CHARTEVENTS.csv\t\t   DRGCODES.csv\t\t   PROCEDUREEVENTS_MV.csv\n",
            "checksum_md5_unzipped.txt  ICUSTAYS.csv\t\t   PROCEDURES_ICD.csv\n",
            "checksum_md5_zipped.txt    INPUTEVENTS_CV.csv\t   README.md\n",
            "CPTEVENTS.csv\t\t   INPUTEVENTS_MV.csv\t   SERVICES.csv\n",
            "DATETIMEEVENTS.csv\t   LABEVENTS.csv\t   SHA256SUMS.txt\n",
            "D_CPT.csv\t\t   LICENSE.txt\t\t   TRANSFERS.csv\n",
            "DIAGNOSES_ICD.csv\t   MICROBIOLOGYEVENTS.csv\n",
            "D_ICD_DIAGNOSES.csv\t   NOTEEVENTS.csv\n"
          ]
        }
      ],
      "source": [
        "!ls $PATH_TO_CSVs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEpOIfJgG4UV",
        "outputId": "85400546-7550-4f03-dd6b-e3d87b3defb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Payel-DLH-related/mimic3-benchmarks\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before running this script, bring in the extract_subjects.py from our github path: DLH-Project-Reproduce-HurtfulWords/mimic3-benchmark/mimic3benchmark/scripts/"
      ],
      "metadata": {
        "id": "7TBgdktXmRMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjccH14QJDFw",
        "outputId": "e7051364-888b-48ad-f77b-2a3d83862893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START:\n",
            "\tICUSTAY_IDs: 61532\n",
            "\tHADM_IDs: 57786\n",
            "\tSUBJECT_IDs: 46476\n",
            "REMOVE ICU TRANSFERS:\n",
            "\tICUSTAY_IDs: 55830\n",
            "\tHADM_IDs: 52834\n",
            "\tSUBJECT_IDs: 43277\n",
            "REMOVE MULTIPLE STAYS PER ADMIT:\n",
            "\tICUSTAY_IDs: 50186\n",
            "\tHADM_IDs: 50186\n",
            "\tSUBJECT_IDs: 41587\n",
            "REMOVE PATIENTS AGE < 18:\n",
            "\tICUSTAY_IDs: 42276\n",
            "\tHADM_IDs: 42276\n",
            "\tSUBJECT_IDs: 33798\n",
            "Using only 1524 ICU stays and 1411 unique patients.\n",
            "Breaking up stays by subjects: 100% 1411/1411 [00:14<00:00, 99.43it/s] \n",
            "Breaking up diagnoses by subjects: 100% 1411/1411 [00:12<00:00, 115.34it/s]\n",
            "Processing CHARTEVENTS table: 100% 330712483/330712484 [30:09<00:00, 182715.33it/s]\n",
            "Processing LABEVENTS table: 100% 27854055/27854056 [02:28<00:00, 187239.63it/s]\n",
            "Processing OUTPUTEVENTS table: 100% 4349218/4349219 [01:04<00:00, 67372.68it/s] \n"
          ]
        }
      ],
      "source": [
        "!python -m mimic3benchmark.scripts.extract_subjects $PATH_TO_CSVs data/root/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ybxzrotC4h"
      },
      "source": [
        "# 2-The following command attempts to fix some issues (ICU stay ID is missing) and removes the events that have missing information. About 80% of events remain after removing all suspicious rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv0F9ZoJs5-L",
        "outputId": "a3175422-cc21-412f-9fb8-eb77053f8259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subjects_root_path='data/root/')\n",
            "Iterating over subjects: 100% 1411/1411 [01:04<00:00, 21.94it/s]\n",
            "n_events: 6637992\n",
            "empty_hadm: 161247\n",
            "no_hadm_in_stay: 2123999\n",
            "no_icustay: 413056\n",
            "recovered: 413056\n",
            "could_not_recover: 0\n",
            "icustay_missing_in_stays: 310324\n"
          ]
        }
      ],
      "source": [
        "# !python -m mimic3benchmark.scripts.validate_events ./data/root/\n",
        "!python -m mimic3benchmark.scripts.validate_events data/root/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7BsfIe2vly2"
      },
      "source": [
        "# 3- The next command breaks up per-subject data into separate episodes (pertaining to ICU stays). Time series of events are stored in {SUBJECT_ID}/episode{#}_timeseries.csv (where # counts distinct episodes) while episode-level information (patient age, gender, ethnicity, height, weight) and outcomes (mortality, length of stay, diagnoses) are stores in {SUBJECT_ID}/episode{#}.csv. This script requires two files, one that maps event ITEMIDs to clinical variables and another that defines valid ranges for clinical variables (for detecting outliers, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVMDxur0vqVe"
      },
      "outputs": [],
      "source": [
        "!python -m mimic3benchmark.scripts.extract_episodes_from_subjects data/root/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfupBxoxv1Ji"
      },
      "source": [
        "#4- The next command splits the whole dataset into training and testing sets. Note that the train/test split is the same of all tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHfu67rlv29x"
      },
      "outputs": [],
      "source": [
        "!python -m mimic3benchmark.scripts.split_train_and_test data/root/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEGbhi2uwC1I"
      },
      "source": [
        "#5- The following commands will generate task-specific datasets, which can later be used in models. These commands are independent, if you are going to work only on one benchmark task, you can run only the corresponding command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YraT4AQ_wGb7",
        "outputId": "cc368880-0693-44d2-cc79-74a60a080b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterating over patients in test:  61% 102/166 [01:32<00:51,  1.24it/s]\n",
            "\t(empty label file) 11245 episode2_timeseries.csv\n",
            "Iterating over patients in test: 100% 166/166 [02:26<00:00,  1.13it/s]\n",
            "Number of created samples: 10715\n",
            "Iterating over patients in train:   6% 69/1227 [01:15<26:56,  1.40s/it]\n",
            "\t(no events in ICU)  2601 episode1_timeseries.csv\n",
            "Iterating over patients in train:  33% 407/1227 [05:06<05:49,  2.35it/s]\n",
            "\t(empty label file) 12607 episode2_timeseries.csv\n",
            "Iterating over patients in train:  41% 499/1227 [06:05<08:06,  1.50it/s]\n",
            "\t(no events in ICU)  16680 episode3_timeseries.csv\n",
            "Iterating over patients in train:  43% 525/1227 [06:24<05:59,  1.95it/s]\n",
            "\t(empty label file) 17269 episode2_timeseries.csv\n",
            "Iterating over patients in train:  50% 613/1227 [07:27<07:25,  1.38it/s]\n",
            "\t(no events in ICU)  18552 episode1_timeseries.csv\n",
            "Iterating over patients in train:  63% 769/1227 [09:05<03:39,  2.09it/s]\n",
            "\t(empty label file) 23407 episode2_timeseries.csv\n",
            "Iterating over patients in train: 100% 1227/1227 [14:03<00:00,  1.46it/s]\n",
            "Number of created samples: 52441\n",
            "Iterating over patients in test: 100% 166/166 [00:55<00:00,  3.02it/s]\n",
            "Number of created samples: 225\n",
            "Iterating over patients in train:   6% 69/1227 [00:18<05:01,  3.84it/s]\n",
            "\t(no events in ICU)  2601 episode1_timeseries.csv\n",
            "Iterating over patients in train:  41% 499/1227 [02:20<03:49,  3.17it/s]\n",
            "\t(no events in ICU)  16680 episode3_timeseries.csv\n",
            "Iterating over patients in train:  50% 613/1227 [02:54<03:42,  2.76it/s]\n",
            "\t(no events in ICU)  18552 episode1_timeseries.csv\n",
            "Iterating over patients in train: 100% 1227/1227 [05:36<00:00,  3.64it/s]\n",
            "Number of created samples: 1261\n",
            "Iterating over patients in test:  61% 102/166 [00:34<00:20,  3.17it/s]\n",
            "\t(empty label file) 11245 episode2_timeseries.csv\n",
            "Iterating over patients in test: 100% 166/166 [00:56<00:00,  2.94it/s]\n",
            "Iterating over patients in train:   6% 69/1227 [00:22<06:24,  3.01it/s]\n",
            "\t(no events in ICU)  2601 episode1_timeseries.csv\n",
            "Iterating over patients in train:  33% 408/1227 [02:12<05:01,  2.72it/s]\n",
            "\t(empty label file) 12607 episode2_timeseries.csv\n",
            "Iterating over patients in train:  41% 499/1227 [02:42<04:04,  2.98it/s]\n",
            "\t(no events in ICU)  16680 episode3_timeseries.csv\n",
            "Iterating over patients in train:  43% 525/1227 [02:50<03:19,  3.52it/s]\n",
            "\t(empty label file) 17269 episode2_timeseries.csv\n",
            "Iterating over patients in train:  50% 613/1227 [03:18<03:30,  2.92it/s]\n",
            "\t(no events in ICU)  18552 episode1_timeseries.csv\n",
            "Iterating over patients in train:  63% 769/1227 [04:10<02:35,  2.95it/s]\n",
            "\t(empty label file) 23407 episode2_timeseries.csv\n",
            "Iterating over patients in train: 100% 1227/1227 [06:38<00:00,  3.08it/s]\n"
          ]
        }
      ],
      "source": [
        "!python -m mimic3benchmark.scripts.create_in_hospital_mortality data/root/ data/in-hospital-mortality/\n",
        "!python -m mimic3benchmark.scripts.create_decompensation data/root/ data/decompensation/\n",
        "!python -m mimic3benchmark.scripts.create_length_of_stay data/root/ data/length-of-stay/\n",
        "!python -m mimic3benchmark.scripts.create_phenotyping data/root/ data/phenotyping/\n",
        "!python -m mimic3benchmark.scripts.create_multitask data/root/ data/multitask/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCyIgC6awf8Y"
      },
      "source": [
        "After the above commands are done, there will be a directory data/{task} for each created benchmark task. These directories have two subdirectories: train and test. Each of them contains a bunch of ICU stays and one file with name listfile.csv, which lists all samples in that particular set. Each row of listfile.csv has the following form: icu_stay, period_length, label(s). A row specifies a sample for which the input is the collection of ICU event of icu_stay that occurred in the first period_length hours of the stay and the target is/are label(s). In in-hospital mortality prediction task period_length is always 48 hours, so it is not listed in corresponding listfiles."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}